# Samba Configuration (3-Loss Hybrid Architecture)

# Model Configuration (130M backbone + decoder)
model:
  # Mamba Backbone (compatible with HuggingFace mamba-130m-hf)
  vocab_size: 50280 # GPT-2 tokenizer
  d_model: 768 # Hidden dimension
  n_layers: 24 # Number of Mamba blocks
  d_state: 16 # SSM state dimension
  d_conv: 4 # Convolution kernel size
  expand_factor: 2 # Expansion factor (d_inner = d_model * expand)
  dt_rank: 48 # Delta rank (ceil(d_model / 16))

  # Decoder configuration (GPT-2 based)
  decoder_n_layers: 6 # Number of decoder layers
  decoder_n_heads: 12 # Number of attention heads
  decoder_window_size: 32 # Windowed self-attention window size
  decoder_dropout: 0.1 # Dropout rate

# Pretrained Model
pretrained:
  use_pretrained: true

  # Mamba backbone weights
  mamba_model: "state-spaces/mamba-130m-hf"

  # Decoder weights (GPT-2 small)
  decoder_model: "gpt2" # Will be adapted for vocab_size=50280

  freeze_backbone: false # Whether to freeze Mamba layers

# Training Configuration
training:
  batch_size: 8 # Adjust based on VRAM (reduce if OOM)
  seq_len: 512
  epochs: 30
  lr: 0.0001
  weight_decay: 0.01
  warmup_ratio: 0.1
  gradient_clip: 1.0

  # 3-Loss weights
  aux_weight: 0.5 # λ_aux - Auxiliary decoder loss weight
  l1_weight: 0.05 # λ_l1 - L1 sparsity loss weight

  # VRAM optimization
  use_gradient_checkpointing: false # Enable if OOM
  use_amp: true # Mixed precision (FP16)

  # Sparsity target (for adaptive pruning)
  target_sparsity: 0.5 # Target 50% sparsity

# Dataset Configuration
dataset:
  name: "wikitext"
  config_name: "wikitext-103-raw-v1"
  tokenizer: "gpt2"
  max_length: 512
  stride: 256 # For sliding window

# Logging
logging:
  use_wandb: false
  project_name: "samba"
  log_interval: 10
  save_dir: "checkpoints"

# Device
device: "cuda" # or "cpu"
seed: 42
