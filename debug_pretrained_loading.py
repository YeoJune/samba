"""
Debug script to verify mamba-ssm parameter loading
ë¹„êµ: HuggingFace Mamba vs Samba model weights
"""

import torch
from transformers import AutoModelForCausalLM
from model.samba import Samba
from utils.pretrained_loader import load_pretrained_samba


def test_full_loading():
    """ì „ì²´ 24ê°œ ë ˆì´ì–´ ë¡œë”© í…ŒìŠ¤íŠ¸ + ì‹¤ì œ ê°’ ë¹„êµ"""
    
    print("="*80)
    print("ğŸ” Step 1: Load models")
    print("="*80)
    
    # 1. HF Mamba ë¡œë“œ
    print("Loading HuggingFace Mamba...")
    hf_model = AutoModelForCausalLM.from_pretrained("state-spaces/mamba-130m-hf")
    hf_state = hf_model.state_dict()
    
    # 2. Samba ìƒì„± (pretrained ì—†ì´)
    print("Creating Samba model...")
    samba_model = Samba(
        vocab_size=50280,
        d_model=768,
        n_layers=24,
        use_cuda=True
    )
    
    print("\n" + "="*80)
    print("ğŸ“Š Step 2: Structure check (layer 0 sample)")
    print("="*80)
    
    layer_0_keys = [k for k in hf_state.keys() if k.startswith("backbone.layers.0.")]
    print(f"HF layer 0 keys: {len(layer_0_keys)}")
    for key in sorted(layer_0_keys)[:3]:
        print(f"  {key}: {hf_state[key].shape}")
    print("  ...")
    
    samba_layer_0_keys = list(samba_model.layers[0].state_dict().keys())
    print(f"\nmamba-ssm layer 0 keys: {len(samba_layer_0_keys)}")
    for key in sorted(samba_layer_0_keys)[:3]:
        print(f"  {key}: {samba_model.layers[0].state_dict()[key].shape}")
    print("  ...")
    
    print("\n" + "="*80)
    print("ğŸ§ª Step 3: Manual loading test (all 24 layers)")
    print("="*80)
    
    # ì „ì²´ ë¡œë”© ì‹œë„
    total_missing = 0
    total_unexpected = 0
    failed_layers = []
    
    for layer_idx in range(24):
        hf_mixer_prefix = f"backbone.layers.{layer_idx}.mixer."
        layer_state_dict = {}
        
        for key, value in hf_state.items():
            if key.startswith(hf_mixer_prefix):
                local_key = key.replace(hf_mixer_prefix, "")
                layer_state_dict[local_key] = value
        
        if len(layer_state_dict) > 0:
            missing, unexpected = samba_model.layers[layer_idx].load_state_dict(
                layer_state_dict, strict=False
            )
            total_missing += len(missing)
            total_unexpected += len(unexpected)
            
            if len(missing) > 0 or len(unexpected) > 0:
                failed_layers.append(layer_idx)
        else:
            failed_layers.append(layer_idx)
            print(f"  âœ— Layer {layer_idx}: No weights found!")
    
    if failed_layers:
        print(f"\nâš ï¸ Failed layers: {failed_layers}")
    else:
        print(f"\nâœ“ All 24 layers loaded successfully!")
    
    print(f"\nTotal missing keys: {total_missing}")
    print(f"Total unexpected keys: {total_unexpected}")
    
    print("\n" + "="*80)
    print("ğŸ”¢ Step 4: Value comparison (layer 0 sample)")
    print("="*80)
    
    # ê°’ ë¹„êµ
    max_diffs = {}
    for key in ['A_log', 'D', 'in_proj.weight', 'out_proj.weight']:
        hf_key = f"backbone.layers.0.mixer.{key}"
        if hf_key in hf_state:
            hf_val = hf_state[hf_key]
            samba_val = samba_model.layers[0].state_dict()[key]
            diff = (hf_val - samba_val).abs().max().item()
            max_diffs[key] = diff
            print(f"  {key}: max_diff = {diff:.6e}")
    
    print("\n" + "="*80)
    print("ğŸ”„ Step 5: Load using pretrained_loader")
    print("="*80)
    
    # ìƒˆë¡œìš´ ëª¨ë¸ë¡œ pretrained_loader í…ŒìŠ¤íŠ¸
    samba_model_loaded = Samba(
        vocab_size=50280,
        d_model=768,
        n_layers=24,
        use_cuda=True
    )
    
    samba_model_loaded = load_pretrained_samba(
        samba_model_loaded,
        pretrained_name="state-spaces/mamba-130m-hf",
        debug=False
    )
    
    print("\n" + "="*80)
    print("âœ… Step 6: Final verification")
    print("="*80)
    
    # ì „ì²´ ë¹„êµ
    print("\nComparing all layer weights...")
    all_match = True
    mismatches = []
    
    for layer_idx in range(24):
        for key in samba_model_loaded.layers[layer_idx].state_dict().keys():
            hf_key = f"backbone.layers.{layer_idx}.mixer.{key}"
            if hf_key in hf_state:
                hf_val = hf_state[hf_key]
                samba_val = samba_model_loaded.layers[layer_idx].state_dict()[key]
                diff = (hf_val - samba_val).abs().max().item()
                
                if diff > 1e-6:
                    all_match = False
                    mismatches.append((layer_idx, key, diff))
    
    if all_match:
        print("âœ“ All weights match perfectly! (diff < 1e-6)")
    else:
        print(f"âš ï¸ Found {len(mismatches)} mismatches:")
        for layer_idx, key, diff in mismatches[:5]:  # Show first 5
            print(f"  Layer {layer_idx}.{key}: diff = {diff:.6e}")
        if len(mismatches) > 5:
            print(f"  ... and {len(mismatches) - 5} more")
    
    # Embedding & norm ì²´í¬
    print("\nChecking embedding & norms...")
    emb_diff = (samba_model_loaded.embedding.weight - hf_model.backbone.embeddings.weight).abs().max().item()
    norm_diff = (samba_model_loaded.norm_f.weight - hf_model.backbone.norm_f.weight).abs().max().item()
    
    print(f"  Embedding: max_diff = {emb_diff:.6e}")
    print(f"  Final norm: max_diff = {norm_diff:.6e}")
    
    # LM head ì²´í¬ (ì¤‘ìš”!)
    print("\nChecking LM head...")
    lm_head_diff = (samba_model_loaded.lm_head.weight - hf_model.lm_head.weight).abs().max().item()
    print(f"  LM head: max_diff = {lm_head_diff:.6e}")
    
    # Weight tying í™•ì¸
    is_tied_samba = (samba_model_loaded.lm_head.weight.data_ptr() == samba_model_loaded.embedding.weight.data_ptr())
    is_tied_hf = (hf_model.lm_head.weight.data_ptr() == hf_model.backbone.embeddings.weight.data_ptr())
    print(f"  Samba weight tying: {'âœ“' if is_tied_samba else 'âœ—'}")
    print(f"  HF weight tying: {'âœ“' if is_tied_hf else 'âœ—'}")
    
    print("\n" + "="*80)
    print("ğŸ“ Summary")
    print("="*80)
    print(f"  Layers loaded: 24/24")
    print(f"  Missing keys: {total_missing}")
    print(f"  Unexpected keys: {total_unexpected}")
    print(f"  Weight mismatches: {len(mismatches)}")
    print(f"  Embedding match: {'âœ“' if emb_diff < 1e-6 else 'âœ—'}")
    print(f"  Norm match: {'âœ“' if norm_diff < 1e-6 else 'âœ—'}")
    print(f"  LM head match: {'âœ“' if lm_head_diff < 1e-6 else 'âœ—'}")
    print(f"  Weight tying: {'âœ“' if is_tied_samba and is_tied_hf else 'âœ—'}")
    
    if len(mismatches) == 0 and emb_diff < 1e-6 and norm_diff < 1e-6 and lm_head_diff < 1e-6:
        print("\nâœ… PASS: Pretrained loading is perfect!")
        return True
    else:
        print("\nâš ï¸ FAIL: Pretrained loading needs fixing!")
        return False


if __name__ == "__main__":
    success = test_full_loading()
